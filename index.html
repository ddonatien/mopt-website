<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GARField: Addressing the visual Sim-to-Real gap in garment manipulation with mesh-attached radiance fields">
  <meta name="keywords" content="NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GARField: Addressing the visual Sim-to-Real gap in garment manipulation with mesh-attached radiance fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4GGH9TN50E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4GGH9TN50E');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GARField: Addressing the visual Sim-to-Real gap in garment manipulation with mesh-attached radiance fields</h1>
          <div class="is-size-3 publication-target">
            <span>ROBIO 2024</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.iit.it/people-details/-/people/donatien-delehelle">Donatien Delehelle</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://www.iit.it/people-details/-/people/darwin-caldwell">Darwin Caldwell</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://feichenlab.com/members/">Fei Chen</a><sup>3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Genova,</span>
            <span class="author-block"><sup>2</sup>Istituto Italiano di Tecnologia,</span>
            <span class="author-block"><sup>3</sup>Chinese University of Hong Kong</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.05038"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=NpEaa2P7qZI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ddonatien/GARField"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">GARField (Garment Attached Radiance Field)</span> addresses the data problem in garment manipulation through learnable, viewpoint-free data generation.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           We present GARField (Garment Attached Radiance Field), the
           first differentiable rendering architecture, to our knowledge,
           for data generation from simulated states stored as triangle
           meshes.
          </p>
          <p>
           While humans intuitively manipulate garments
           and other textile items swiftly and accurately, it is a significant
           challenge for robots. A factor crucial to human performance
           is the ability to imagine, a priori, the intended result of the
           manipulation intents and hence develop predictions on the garment pose.
           That ability allows us to plan from highly obstructed
           states, adapt our plans as we collect more information and react
           swiftly to unforeseen circumstances. Conversely, robots struggle
           to establish such intuitions and form tight links between plans
           and observations.
          </p>
          <p>
           We can partly attribute this to the high
           cost of obtaining densely labelled data for textile manipulation,
           both in quality and quantity. The problem of data collection
           is a long-standing issue in data-based approaches to garment
           manipulation. As of today, generating high-quality and labelled
           garment manipulation data is mainly attempted through advanced
           data capture procedures that create a simplified state
           estimations from real-world observations. 
          </p>
          <p>
           However, this work
           proposes a novel approach to the problem by generating
           real-world observations from object states.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Video generation</h2>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/sock_fall_render.mp4"
                  type="video/mp4">
          <source src="./static/videos/sock_fall_render.webm"
                  type="video/webm">
          <source src="./static/videos/sock_fall_render.ogv"
                  type="video/ogg">
          Your browser doesn't seem to support the video tag.
          </video>
        </div>
      </div>
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Re-posed mesh image</h2>
          <img src="./static/images/image_gen.png" class="reposed_image" alt="Image of re-posed mesh" width="320px"/>
        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Overview</h2>

        <div class="content has-text-justified">
          <p>
            GARField models the scene as a composition of signed distance and visual feature fields. The background field is defined in the
            scene’s global coordinates frame. The other fields are attached to objects’ meshes and can be re-posed. The mesh-attached coordinates
            system projects query points in a coordinate system made up of the point’s distance to the mesh’s surface and coordinates of the surface-projection
            of the query point in a bespoke coordinate system built around Laplacian-based position embeddings and barycentric coordinates.
          </p>
        </div>
        <div class="columns is-vcentered overview-image">
            <img src="./static/images/overview.png"
                 class="overview-image"
                 alt="Architecture overview image."/>
        </div>

        <h2 class="title is-3">Reconstruction performance</h2>

        <div class="columns is-vcentered overview-image">
            <img src="./static/images/qual_1.png"
                 class="overview-image"
                 alt="Architecture overview image."/>
        </div>
        <br/>
      </div>
    </div>

    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

         <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
 <div class="container is-max-desktop content">
   <h2 class="title">BibTeX</h2>
   <pre><code>@article{delehelle2024garfield,
  title={GARField: Addressing the visual Sim-to-Real gap in garment manipulation with mesh-attached radiance fields},
  author={Delehelle, Donatien and Caldwell, Darwin G and Chen, Fei},
  journal={arXiv preprint arXiv:2410.05038},
  year={2024}
}</code></pre>
<!--  </div> -->
<!--</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/ddonatien" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The template of this website was borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>. Check them out !
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
